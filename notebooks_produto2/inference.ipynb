{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência\n",
    "\n",
    "Neste notebook, vamos aplicar modelos treinado no conjunto de teste\n",
    "\n",
    "Precisa ter os modelos gerados na pasta /models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"{prefix} Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"{prefix} Memory Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import src.data.preprocess_data as data\n",
    "import src.training.train_model as train\n",
    "import src.data.view as view\n",
    "import src.models.unets as unets\n",
    "import src.models.hrnets as hrnets\n",
    "import src.training.post_processing as post\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleção do modelo, deve estar na pasta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UNetSmall-256-type-CE-diceW.pth'\n",
    "\n",
    "model_name = 'UNetSmall-64-type-CEW.pth' #predicts lots of 4\n",
    "model_name = 'UNetConvNext-224-type-DS-CEW.pth' #do teh opposite, predicts as otehr classes\n",
    "model_name = 'UNetSmall-256-type-DS-CEW.pth' #do teh opposite, predicts as otehr classes\n",
    "model_name = 'UNet-256-type-DS-CE.pth'\n",
    "#model_name = 'UNetConvNext-224-type-CEW.pth'\n",
    "#model_name = 'HRNetW32-512-type-CEW.pth'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega-se os parâmetros de acordo com o nome do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_size = int(model_name.split('-')[1])\n",
    "weighted = 'W.pth' in model_name\n",
    "if model_name.split('-')[2]=='type':\n",
    "    num_classes = 5\n",
    "if model_name.split('-')[2]=='binary':\n",
    "    num_classes = 2\n",
    "loss_mode = model_name.split('-')[-1].split('.')[0]\n",
    "if loss_mode.endswith('W'):\n",
    "    loss_mode = loss_mode[:-1]\n",
    "crf=False\n",
    "dist=False\n",
    "if model_name.split('-')[3]=='crf':\n",
    "    crf = True\n",
    "if model_name.split('-')[3]=='dist':\n",
    "    dist = True\n",
    "\n",
    "criterion = train.CombinedLoss(loss_mode = loss_mode, weights = None, return_all_losses=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancia e Carrega o checkpoint do modelo, de acordo com o prefixo dele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "working_dir = os.path.abspath('..')\n",
    "ckp_file = os.path.join(working_dir, 'models', model_name)\n",
    "\n",
    "print('Model name:', model_name)\n",
    "if model_name.startswith('UNetSmall-'):\n",
    "    model = unets.UNetSmall(in_channels=12, out_channels=num_classes, crf=crf, use_dist=dist).to(device) \n",
    "if model_name.startswith('UNet-'):\n",
    "    model = unets.UNet(in_channels=12, out_channels=num_classes).to(device) \n",
    "elif model_name.startswith('UNetResNet34-'):\n",
    "    model = unets.UNetResNet34(in_channels=12, out_channels=num_classes).to(device) \n",
    "elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "    model = unets.UNetEfficientNetB0(in_channels=12, out_channels=num_classes).to(device) \n",
    "elif model_name.startswith('UNetConvNext-'):\n",
    "    model = unets.UNetConvNext(in_channels=12, out_channels=num_classes).to(device) \n",
    "elif model_name.startswith('HRNetW18'):\n",
    "    model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "elif model_name.startswith('HRNetW32'):\n",
    "    model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "elif model_name.startswith('HRNetW48'):\n",
    "    model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "checkpoint = torch.load(ckp_file, weights_only=False)\n",
    "        \n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_epoch = checkpoint['best_epoch']\n",
    "model.load_state_dict(checkpoint['best_model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['best_optimizer_state_dict']) \n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "best_epoch_info = checkpoint['best_epoch_info']\n",
    "current_lr = checkpoint['current_lr']\n",
    "current_patience = checkpoint['current_parience']       \n",
    "metadata = checkpoint['metadata']\n",
    "info = checkpoint['best_epoch_info']\n",
    "history = checkpoint['history']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra as estatisticas do treinamento (uma época)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in history:\n",
    "    print(info['train_acc'])\n",
    "    print(info)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_col(history, column):\n",
    "\n",
    "    train_metric = [info[f'train_{column}'] for info in history]\n",
    "    val_metric = [info[f'val_{column}'] for info in history]\n",
    "    print(train_metric)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    ax.plot(epochs, train_metric, label=f\"Training {column}\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    ax.plot(epochs, val_metric, label=f\"Validation {column}\", marker=\"o\", linestyle=\"-\", color=\"red\")\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    ax.set_title(f\"Training and Validation {column}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(column)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "columns = ['loss', 'acc', 'micro', 'macro', 'weighted', 'f1_C0', 'f1_C1', 'f1_C2', 'f1_C3', 'f1_C4', 'CE', 'dice']    \n",
    "#for c in columns:\n",
    "#    plot_col(history, c)\n",
    "\n",
    "def get_best(history):\n",
    "    lowest_loss = np.inf\n",
    "    info_best = {}\n",
    "    for info in history:\n",
    "        if info['val_loss'] <= lowest_loss:\n",
    "            lowest_loss = info['val_loss']\n",
    "            info_best = info\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiza o loss pelas épocas, do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['loss', 'acc', 'micro', 'macro', 'weighted', 'f1_C0', 'f1_C1', 'f1_C2', 'f1_C3', 'f1_C4', 'CE', 'dice']   \n",
    "for c in columns:\n",
    "    view.plot_metrics(history, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação no conjunto de teste\n",
    "\n",
    "Definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = ['032027']#, '032026'] \n",
    "num_subtiles = 6\n",
    "classes_mode = 'type'\n",
    "training_batch_size = 16\n",
    "model_types = 'unets'\n",
    "weighted = True\n",
    "\n",
    "if classes_mode == 'type':\n",
    "    num_classes = 5\n",
    "elif classes_mode == 'density':\n",
    "    num_classes = 4\n",
    "elif classes_mode == 'binary':\n",
    "    num_classes = 2\n",
    "elif classes_mode == 'all':\n",
    "    num_classes = 9\n",
    "\n",
    "\n",
    "working_dir = os.path.abspath('..')\n",
    "models_paths = os.listdir(os.path.join(working_dir, 'models'))\n",
    "models_paths = [f for f in models_paths if (f.endswith('CE.pth') or f.endswith('CEW.pth'))]\n",
    "models_paths.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = classes_mode,\n",
    "                                                                    subfolder='q_12ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de teste\n",
    "\n",
    "Criamos estrutura de dados que armazena as métricas relacionadas ao recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'recall'\n",
    "highest_binary_macro = {'model':'', 'value': 0.0}\n",
    "highest_4class_macro = {'model':'', 'value': 0.0}\n",
    "highest_5class_macro = {'model':'', 'value': 0.0}\n",
    "\n",
    "highest_binary_weighted = {'model':'', 'value': 0.0}\n",
    "highest_4class_weighted = {'model':'', 'value': 0.0}\n",
    "highest_5class_weighted = {'model':'', 'value': 0.0}\n",
    "\n",
    "highest_binary_global = {'model':'', 'value': 0.0}\n",
    "highest_4class_global = {'model':'', 'value': 0.0}\n",
    "highest_5class_global = {'model':'', 'value': 0.0}\n",
    "\n",
    "binary_labels = [\"(0,3)\", \"(1,2,4)\"]\n",
    "join_labels = [\"(0,3)\", \"(1)\", \"(2)\", \"(4)\"]\n",
    "class_labels = list(range(num_classes))\n",
    "class_labels = [str(cl) for cl in class_labels]\n",
    "\n",
    "highest_binary = [{'model':'', 'class':'(0,3)','value': 0.0}, {'model':'', 'class':'(1,2,4)', 'value': 0.0}]\n",
    "highest_4class = [{'model':'', 'class':'(0,3)', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "highest_5class = [{'model':'', 'class':'0', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'3', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "\n",
    "\n",
    "highest_precision_global = {'model':'', 'value': 0.0}\n",
    "highest_f1_global = {'model':'', 'value': 0.0}\n",
    "highest_precision_macro = {'model':'', 'value': 0.0}\n",
    "highest_f1_macro = {'model':'', 'value': 0.0}\n",
    "highest_precision = [{'model':'', 'class':'0', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'3', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "highest_f1 = [{'model':'', 'class':'0', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'3', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "\n",
    "ordered_f1s = {'macro avg':[], 'weighted avg': [], 'Class 0':[], 'Class 1':[], 'Class 2':[], 'Class 3':[], 'Class 4':[]} \n",
    "ordered_recalls = {'macro avg':[], 'weighted avg': [], 'Class 0':[], 'Class 1':[], 'Class 2':[], 'Class 3':[], 'Class 4':[]} \n",
    "ordered_precisions = {'macro avg':[], 'weighted avg': [], 'Class 0':[], 'Class 1':[], 'Class 2':[], 'Class 3':[], 'Class 4':[]} \n",
    "\n",
    "all_reports = {}\n",
    "for model_name in [mp for mp in models_paths]:# if 'UNet-256-type-DS-CEW' in mp]:#model_paths:\n",
    "#for model_name in model_paths:#[mp for mp in models_paths if 'UNet-256-type-DS-CEW' in mp]:#model_paths:\n",
    "    print('Model name:', model_name)\n",
    "    if model_name.startswith('UNetSmall-'):\n",
    "        model = unets.UNetSmall(in_channels=12, out_channels=num_classes, crf=crf, use_dist=dist).to(device) \n",
    "    elif model_name.startswith('UNet-'):\n",
    "        model = unets.UNet(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetResNet34-'):\n",
    "        model = unets.UNetResNet34(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "        model = unets.UNetEfficientNetB0(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetConvNext-'):\n",
    "        model = unets.UNetConvNext(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('HRNetW18'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW32'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW48'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "    else:\n",
    "        print('Nao existe esse modelo')\n",
    "        continue\n",
    "    checkpoint = torch.load(ckp_file, weights_only=False)\n",
    "\n",
    "    if model_name.startswith('UNet'):\n",
    "        BS = 16\n",
    "    elif model_name.startswith('HRNet'):\n",
    "        BS = 4\n",
    "    else:\n",
    "        print('não existe esse modelo')\n",
    "        continue\n",
    "\n",
    "    yaml_filename = data.yaml_filename(num_subtiles, tiles, classes_mode)\n",
    "    print(yaml_filename)\n",
    "    print('----------------')\n",
    "    test_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'test_files',\n",
    "                                    patch_size=patch_size, \n",
    "                                    stride=patch_size, \n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    )\n",
    "\n",
    "    dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False)\n",
    "\n",
    "    cm, report = train.test_model(model, \n",
    "                    checkpoint_path=model_name,\n",
    "                    dataloader = dataloader, \n",
    "                    device = device, \n",
    "                    num_classes = num_classes,\n",
    "                    set_name = '-test-032027'\n",
    "                    )\n",
    "    all_reports[model_name] = report\n",
    "    if 0:\n",
    "        print(recalls['binary']) \n",
    "        print(recalls['binary']['macro_recall'])\n",
    "        print(highest_binary_macro['value'])\n",
    "        if recalls['binary']['macro_recall']>highest_binary_macro['value']:\n",
    "            highest_binary_macro  = {'model':model_name, 'value':recalls['binary']['macro_recall']}\n",
    "        if recalls['binary']['weighted_recall']>highest_binary_weighted['value']:\n",
    "            highest_binary_weighted = {'model':model_name, 'value':recalls['binary']['weighted_recall']}\n",
    "        if recalls['binary']['global_recall']>highest_binary_global['value']:\n",
    "            highest_binary_global = {'model':model_name, 'value':recalls['binary']['global_recall']}\n",
    "\n",
    "        if recalls['4class']['macro_recall']>highest_4class_macro['value']:\n",
    "            highest_4class_macro = {'model':model_name, 'value':recalls['4class']['macro_recall']}\n",
    "        if recalls['4class']['weighted_recall']>highest_4class_weighted['value']:\n",
    "            highest_4class_weighted = {'model':model_name, 'value':recalls['4class']['weighted_recall']}\n",
    "        if recalls['4class']['global_recall']>highest_4class_global['value']:\n",
    "            highest_4class_global = {'model':model_name, 'value':recalls['4class']['global_recall']}\n",
    "\n",
    "        if recalls['5class']['macro_recall']>highest_5class_macro['value']:\n",
    "            highest_5class_macro = {'model':model_name, 'value':recalls['5class']['macro_recall']}\n",
    "        if recalls['5class']['weighted_recall']>highest_5class_weighted['value']:\n",
    "            highest_5class_weighted = {'model':model_name, 'value':recalls['5class']['weighted_recall']}\n",
    "        if recalls['5class']['global_recall']>highest_5class_global['value']:\n",
    "            highest_5class_global = {'model':model_name, 'value':recalls['5class']['global_recall']}\n",
    "\n",
    "\n",
    "        for i, bl in enumerate(binary_labels):\n",
    "            if recalls['binary'][bl+'_recall']>highest_binary[i]['value']:\n",
    "                highest_binary[i]['model'] = model_name\n",
    "                highest_binary[i]['value'] = recalls['binary'][bl+'_recall']\n",
    "        for i, jl in enumerate(join_labels):\n",
    "            if recalls['4class'][jl+'_recall']>highest_4class[i]['value']:\n",
    "                highest_4class[i]['model'] = model_name\n",
    "                highest_4class[i]['value'] = recalls['4class'][jl+'_recall']\n",
    "        for i, cl in enumerate(class_labels):\n",
    "            if recalls['5class'][cl+'_recall']>highest_5class[i]['value']:\n",
    "                highest_5class[i]['model'] = model_name\n",
    "                highest_5class[i]['value'] = recalls['5class'][cl+'_recall']\n",
    "\n",
    "    if report['recall']['weighted avg']>highest_precision_global['value']:\n",
    "        highest_precision_global = {'model':model_name, 'value':report['recall']['weighted avg']}\n",
    "    if report['recall']['macro avg']>highest_precision_macro['value']:\n",
    "        highest_precision_macro = {'model':model_name, 'value':report['recall']['macro avg']}\n",
    "    if report['precision']['weighted avg']>highest_precision_global['value']:\n",
    "        highest_precision_global = {'model':model_name, 'value':report['precision']['weighted avg']}\n",
    "    if report['precision']['macro avg']>highest_precision_macro['value']:\n",
    "        highest_precision_macro = {'model':model_name, 'value':report['precision']['macro avg']}\n",
    "    if report['f1-score']['weighted avg']>highest_f1_global['value']:\n",
    "        highest_f1_global = {'model':model_name, 'value':report['f1-score']['weighted avg']}\n",
    "    if report['f1-score']['macro avg']>highest_f1_macro['value']:\n",
    "        highest_f1_macro = {'model':model_name, 'value':report['f1-score']['macro avg']}\n",
    "\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        if report['f1-score'][f'Class {i}']>highest_f1[i]['value']:\n",
    "            highest_f1[i]['model'] = model_name\n",
    "            highest_f1[i]['value'] = report['f1-score'][f'Class {i}']\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        if report['precision'][f'Class {i}']>highest_precision[i]['value']:\n",
    "            highest_precision[i]['model'] = model_name\n",
    "            highest_precision[i]['value'] = report['precision'][f'Class {i}']\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        if report['recall'][f'Class {i}']>highest_precision[i]['value']:\n",
    "            highest_precision[i]['model'] = model_name\n",
    "            highest_precision[i]['value'] = report['recall'][f'Class {i}']\n",
    "\n",
    "\n",
    "    ordered_f1s['weighted avg'].append({'model': model_name, 'value': report['f1-score']['weighted avg']})\n",
    "    ordered_f1s['macro avg'].append({'model': model_name, 'value': report['f1-score']['macro avg']})\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        ordered_f1s[f'Class {i}'].append({'model': model_name, 'value': report['f1-score'][f'Class {i}']})\n",
    "    \n",
    "    ordered_recalls['weighted avg'].append({'model': model_name, 'value': report['recall']['weighted avg']})\n",
    "    ordered_recalls['macro avg'].append({'model': model_name, 'value': report['recall']['macro avg']})\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        ordered_recalls[f'Class {i}'].append({'model': model_name, 'value': report['recall'][f'Class {i}']})\n",
    "\n",
    "    ordered_precisions['weighted avg'].append({'model': model_name, 'value': report['precision']['weighted avg']})\n",
    "    ordered_precisions['macro avg'].append({'model': model_name, 'value': report['precision']['macro avg']})\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        ordered_precisions[f'Class {i}'].append({'model': model_name, 'value': report['precision'][f'Class {i}']})\n",
    "\n",
    "\n",
    "\n",
    "ordered_f1s['weighted avg']=sorted(ordered_f1s['weighted avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "ordered_f1s['macro avg']=sorted(ordered_f1s['macro avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "for i, cl in enumerate(class_labels):\n",
    "    ordered_f1s[f'Class {i}'] = sorted(ordered_f1s[f'Class {i}'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "    \n",
    "ordered_recalls['weighted avg']=sorted(ordered_recalls['weighted avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "ordered_recalls['macro avg']=sorted(ordered_recalls['macro avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "for i, cl in enumerate(class_labels):\n",
    "    ordered_recalls[f'Class {i}'] = sorted(ordered_recalls[f'Class {i}'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "\n",
    "ordered_precisions['weighted avg']=sorted(ordered_precisions['weighted avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "ordered_precisions['macro avg']=sorted(ordered_precisions['macro avg'], key=lambda x: x['value'], reverse=True)[:5]\n",
    "for i, cl in enumerate(class_labels):\n",
    "    ordered_precisions[f'Class {i}'] = sorted(ordered_precisions[f'Class {i}'], key=lambda x: x['value'], reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Binary macro:', highest_binary_macro)\n",
    "print('Binary weighted:', highest_binary_weighted)\n",
    "print('Binary global:', highest_binary_global)\n",
    "for i, bl in enumerate(binary_labels):\n",
    "    print(f'Binary Recall, class {bl}', highest_binary[i])\n",
    "print()\n",
    "print('4 class macro:', highest_4class_macro)\n",
    "print('4 class weighted:', highest_4class_weighted)\n",
    "print('4 class global:', highest_4class_global)\n",
    "for i, jl in enumerate(join_labels):\n",
    "    print(f'4 class Recall, class {jl}', highest_4class[i])\n",
    "print()\n",
    "print('5 class macro:', highest_5class_macro)\n",
    "print('5 class weighted:', highest_5class_weighted)\n",
    "print('5 class global:', highest_5class_global)\n",
    "for i, cl in enumerate(class_labels):\n",
    "    print(f'5 class Recall, class {cl}', highest_5class[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1-score: ')\n",
    "print('macro:', highest_f1_macro)\n",
    "print('global:', highest_f1_global)\n",
    "for i, cl in enumerate(class_labels):\n",
    "    print(f'f1, class {cl}', highest_f1[i])\n",
    "\n",
    "print('precision: ')\n",
    "print('macro:', highest_precision_macro)\n",
    "print('global:', highest_precision_global)\n",
    "for i, cl in enumerate(class_labels):\n",
    "    print(f'precision, class {cl}', highest_precision[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = list(all_reports.keys())  # X-axis labels\n",
    "\n",
    "recalls = []\n",
    "rec_macro = []\n",
    "rec_wei = []\n",
    "\n",
    "for k,v in all_reports.items():\n",
    "    print(k)\n",
    "    prec=[]\n",
    "    rec = []\n",
    "    prec_t=[]\n",
    "    rec_t = []\n",
    "    for i in range(5):\n",
    "        rec.append(v['recall'][f'Class {i}'])\n",
    "    recalls.append(rec)\n",
    "    \n",
    "print(v['recall'])\n",
    "suffixes = ['-type-CE.pth', '-type-CEW.pth', '-type-DS-CE.pth', '-type-DS-CEW.pth']\n",
    "save_to = os.path.join(working_dir,\"figs\",\"test_recall.png\")\n",
    "view.plot_metric(recalls, names, suffixes, metric_name=\"Recall\", save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão por conjunto de classes (binária, 4 classes e 5 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = 'type')\n",
    "\n",
    "\n",
    "highest_binary_macro = {'model':'', 'value': 0.0}\n",
    "highest_4class_macro = {'model':'', 'value': 0.0}\n",
    "highest_5class_macro = {'model':'', 'value': 0.0}\n",
    "\n",
    "highest_binary_weighted = {'model':'', 'value': 0.0}\n",
    "highest_4class_weighted = {'model':'', 'value': 0.0}\n",
    "highest_5class_weighted = {'model':'', 'value': 0.0}\n",
    "\n",
    "highest_binary_global = {'model':'', 'value': 0.0}\n",
    "highest_4class_global = {'model':'', 'value': 0.0}\n",
    "highest_5class_global = {'model':'', 'value': 0.0}\n",
    "\n",
    "\n",
    "binary_labels = [\"(0,3)\", \"(1,2,4)\"]\n",
    "join_labels = [\"(0,3)\", \"(1)\", \"(2)\", \"(4)\"]\n",
    "class_labels = list(range(num_classes))\n",
    "class_labels = [str(cl) for cl in class_labels]\n",
    "\n",
    "highest_binary = [{'model':'', 'class':'(0,3)','value': 0.0}, {'model':'', 'class':'(1,2,4)', 'value': 0.0}]\n",
    "highest_4class = [{'model':'', 'class':'(0,3)', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "highest_5class = [{'model':'', 'class':'0', 'value': 0.0}, {'model':'', 'class':'1', 'value': 0.0}, {'model':'', 'class':'2', 'value': 0.0}, {'model':'', 'class':'3', 'value': 0.0}, {'model':'', 'class':'4', 'value': 0.0}]\n",
    "\n",
    "for model_name in [mp for mp in models_paths if 'UNet-256-type-DS-CEW' in mp]:#model_paths:\n",
    "#for model_name in models_paths:\n",
    "    print('Model name:', model_name)\n",
    "    if model_name.startswith('UNetSmall-'):\n",
    "        model = unets.UNetSmall(in_channels=12, out_channels=num_classes, crf=crf, use_dist=dist).to(device) \n",
    "    elif model_name.startswith('UNet-'):\n",
    "        model = unets.UNet(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetResNet34-'):\n",
    "        model = unets.UNetResNet34(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "        model = unets.UNetEfficientNetB0(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('UNetConvNext-'):\n",
    "        model = unets.UNetConvNext(in_channels=12, out_channels=num_classes).to(device) \n",
    "    elif model_name.startswith('HRNetW18'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW32'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW48'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "    else:\n",
    "        print('Nao existe esse modelo')\n",
    "        continue\n",
    "    checkpoint = torch.load(ckp_file, weights_only=False)\n",
    "\n",
    "    if model_name.startswith('UNet'):\n",
    "        BS = 16\n",
    "    elif model_name.startswith('HRNet'):\n",
    "        BS = 4\n",
    "    else:\n",
    "        print('não existe esse modelo')\n",
    "        continue\n",
    "\n",
    "    full_dataset = data.SubtileDataset(train_files+val_files+test_files, \n",
    "                                            num_subtiles = num_subtiles, \n",
    "                                            classes_mode = 'type', \n",
    "                                            patch_size = patch_size, \n",
    "                                            stride=patch_size, # sem overlap\n",
    "                                            dynamic_sampling = False,\n",
    "                                            data_augmentation = False\n",
    "                                           )\n",
    "\n",
    "    dataloader = DataLoader(full_dataset, batch_size=BS, shuffle=False)\n",
    "\n",
    "    recalls = train.test_model(model, \n",
    "                    checkpoint_path=model_name,\n",
    "                    dataloader = dataloader, \n",
    "                    device = device, \n",
    "                    num_classes = num_classes,\n",
    "                    set_name = '-full-032027'\n",
    "                    ) \n",
    "    print(recalls)\n",
    "    if recalls['binary']['macro_recall']>highest_binary_macro['value']:\n",
    "        highest_binary_macro = {'model':model_name, 'value':recalls['binary']['macro_recall']}\n",
    "    if recalls['binary']['weighted_recall']>highest_binary_weighted['value']:\n",
    "        highest_binary_weighted = {'model':model_name, 'value':recalls['binary']['weighted_recall']}\n",
    "    if recalls['binary']['global_recall']>highest_binary_global['value']:\n",
    "        highest_binary_global = {'model':model_name, 'value':recalls['binary']['global_recall']}\n",
    "\n",
    "    if recalls['4class']['macro_recall']>highest_4class_macro['value']:\n",
    "        highest_4class_macro = {'model':model_name, 'value':recalls['4class']['macro_recall']}\n",
    "    if recalls['4class']['weighted_recall']>highest_4class_weighted['value']:\n",
    "        highest_4class_weighted = {'model':model_name, 'value':recalls['4class']['weighted_recall']}\n",
    "    if recalls['4class']['global_recall']>highest_4class_global['value']:\n",
    "        highest_4class_global = {'model':model_name, 'value':recalls['4class']['global_recall']}\n",
    "\n",
    "    if recalls['5class']['macro_recall']>highest_5class_macro['value']:\n",
    "        highest_5class_macro = {'model':model_name, 'value':recalls['5class']['macro_recall']}\n",
    "    if recalls['5class']['weighted_recall']>highest_5class_weighted['value']:\n",
    "        highest_5class_weighted = {'model':model_name, 'value':recalls['5class']['weighted_recall']}\n",
    "    if recalls['5class']['global_recall']>highest_5class_global['value']:\n",
    "        highest_5class_global = {'model':model_name, 'value':recalls['5class']['global_recall']}\n",
    "\n",
    "\n",
    "    for i, bl in enumerate(binary_labels):\n",
    "        if recalls['binary'][bl+'_recall']>highest_binary[i]['value']:\n",
    "            highest_binary[i]['model'] = model_name\n",
    "            highest_binary[i]['value'] = recalls['binary'][bl+'_recall']\n",
    "    for i, jl in enumerate(join_labels):\n",
    "        if recalls['4class'][jl+'_recall']>highest_4class[i]['value']:\n",
    "            highest_4class[i]['model'] = model_name\n",
    "            highest_4class[i]['value'] = recalls['4class'][jl+'_recall']\n",
    "    for i, cl in enumerate(class_labels):\n",
    "        if recalls['5class'][cl+'_recall']>highest_5class[i]['value']:\n",
    "            highest_5class[i]['model'] = model_name\n",
    "            highest_5class[i]['value'] = recalls['5class'][cl+'_recall']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Binary macro:', highest_binary_macro)\n",
    "print('Binary weighted:', highest_binary_weighted)\n",
    "print('Binary global:', highest_binary_global)\n",
    "for i, bl in enumerate(binary_labels):\n",
    "    print(f'Binary Recall, class {bl}', highest_binary[i])\n",
    "print()\n",
    "print('4 class macro:', highest_4class_macro)\n",
    "print('4 class weighted:', highest_4class_weighted)\n",
    "print('4 class global:', highest_4class_global)\n",
    "for i, jl in enumerate(join_labels):\n",
    "    print(f'4 class Recall, class {jl}', highest_4class[i])\n",
    "print()\n",
    "print('5 class macro:', highest_5class_macro)\n",
    "print('5 class weighted:', highest_5class_weighted)\n",
    "print('5 class global:', highest_5class_global)\n",
    "for i, cl in enumerate(class_labels):\n",
    "    print(f'5 class Recall, class {cl}', highest_5class[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos com a limpeza morfológica da inferência do conjunto de teste teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = '032027'\n",
    "\n",
    "folder = os.path.join(working_dir,f\"data/processed/S2-16D_V2_{tile}/{num_subtiles}x{num_subtiles}_subtiles\")\n",
    "files = os.listdir(folder)\n",
    "files = [os.path.join(folder, f) for f in files if f.endswith('.tif')]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = patch_size-32\n",
    "edge_removal = 8\n",
    "if patch_size == 64:\n",
    "    stride = patch_size-16\n",
    "    edge_removal = 4\n",
    "\n",
    "for tile_id in ['032027']:#, '025037', '032027']:\n",
    "    # --------------- opening files -----------------\n",
    "    folder = os.path.join(working_dir,f\"data/processed/S2-16D_V2_{tile_id}/{num_subtiles}x{num_subtiles}_subtiles\")\n",
    "    files = os.listdir(folder)\n",
    "    files = [os.path.join(folder, f) for f in files if f.endswith('.tif')]\n",
    "    # --------------- creating a dataloader -----------------\n",
    "    test_dataset = data.SubtileDataset(files, \n",
    "                                    num_subtiles = 6,\n",
    "                                    classes_mode=classes_mode,\n",
    "                                    patch_size=patch_size, \n",
    "                                    stride=stride, #//2, \n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    return_imgidx = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=training_batch_size, shuffle=False)\n",
    "    #for image, mask, x, y, f in test_loader:\n",
    "    #    print(f'{x},{y}',end='|')\n",
    "    tile = post.ReconstructTile(patch_size = patch_size, stride = stride, edge_removal=edge_removal)\n",
    "\n",
    "    ### -------------- TESTING -------------------\n",
    "    import time\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    run_time = time.time()\n",
    "    runner = train.EpochRunner('test', model, test_loader, criterion=criterion, num_classes=num_classes, \n",
    "                                optimizer=None, simulated_batch_size = test_loader.batch_size, device = device)  \n",
    "    for image, label, logits, pred, x, y, f, in runner.run_generator(show_pred = 1):\n",
    "        tile.add_batch(x, y, f, logits, pred, label, image)\n",
    "    print('Montando')\n",
    "    tile.set_pred()      \n",
    "        \n",
    "    loss, CE, dice, report, acc, cm = runner.get_metrics()\n",
    "    run_time = time.time()-run_time\n",
    "    peak_train_memory = f\"{torch.cuda.max_memory_allocated() / 1024 ** 2:.2f} MB\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    print(f'Test Loss: {loss}, {CE}, {dice}')\n",
    "    print(f'Test Accuracy: {acc}')\n",
    "    print(f'Test confusion matrix:')\n",
    "    view.plot_confusion_matrix(cm)\n",
    "    print(report)\n",
    "    \n",
    "    #### ----------------- Contruct tile\n",
    "    \n",
    "    \n",
    "\n",
    "    r = [0, 10560, 0, 10560]\n",
    "    #r = [5000, 7000, 1000, 3000]\n",
    "\n",
    "    plt.figure(figsize=(50, 50))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tile.labels[r[0]:r[1], r[2]:r[3]])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tile.preds[r[0]:r[1], r[2]:r[3]])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(1504+256)\n",
    "#plt.figure(figsize=(15,15))\n",
    "#plt.imshow(prob[4,230:280, 230:280])\n",
    "labels, pred_patch, clean_pred, clean_noholes, clean_noholes_2, noholes, noholes2, rules = tile.post_process(0,0)\n",
    "titles = [\"Referência\", \"Predição do modelo\", \"CRF\", \"CRF + buracos preenchidos\", \"CRF + buracos preenchidos 2\", \"Com buracos preenchidos\", \"Com buracos preenchidos 2\", \"Regras\"]\n",
    "view.plot_mask_list([labels, pred_patch, clean_pred, clean_noholes, clean_noholes_2, noholes, noholes2, rules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean, crf, Q, clean_crf, \n",
    "\n",
    "label_map = {\n",
    "        0: ('Reds', 0.0, 'Fundo'),\n",
    "        1: ('Blues', 0.8, 'Loteamento Vazio'),\n",
    "        2: ('Greens', 0.8, 'Outros Equipamentos'),\n",
    "        3: ('Reds', 0.8, 'Vazio Intraurbano'),\n",
    "        4: ('Oranges', 0.8, 'Área Urbanizada')\n",
    "    }\n",
    "fig, axs = plt.subplots(4, 2, figsize=(80, 20))\n",
    "axs = axs.flatten()\n",
    "print(axs)\n",
    "view.plot_masked_image(labels, label_map, image=None, title=\"Original\", ax=axs[0])\n",
    "view.plot_masked_image(pred, label_map, image=None, title=\"Original\", ax=axs[1])\n",
    "view.plot_masked_image(clean_pred, label_map, image=None, title=\"Original\", ax=axs[2])\n",
    "view.plot_masked_image(vazios, label_map, image=None, title=\"Original\", ax=axs[3])\n",
    "view.plot_masked_image(crfc1, label_map, image=None, title=\"Original\", ax=axs[4])\n",
    "view.plot_masked_image(crfc1, label_map, image=None, title=\"Original\", ax=axs[5])\n",
    "view.plot_masked_image(c1, label_map, image=None, title=\"Original\", ax=axs[6])\n",
    "view.plot_masked_image(c2, label_map, image=None, title=\"Original\", ax=axs[7])\n",
    "\n",
    "if 0:\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    plt.subplot(4,2,1)\n",
    "    plt.imshow(labels)\n",
    "    plt.subplot(4,2,2)\n",
    "    plt.imshow(pred)\n",
    "    plt.subplot(4,2,3)\n",
    "    plt.imshow(clean_pred)\n",
    "    plt.subplot(4,2,4)\n",
    "    plt.imshow(vazios)\n",
    "    plt.subplot(4,2,5)\n",
    "    plt.imshow(crfc1)\n",
    "    plt.subplot(4,2,6)\n",
    "    plt.imshow(crfc2)\n",
    "    plt.subplot(4,2,7)\n",
    "    plt.imshow(c1)\n",
    "    plt.subplot(4,2,8)\n",
    "    plt.imshow(c2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "def remove_small(final_mask, min_size = 10):\n",
    "    final_mask = remove_small_objects(final_mask, min_size=min_size)\n",
    "    return final_mask\n",
    "\n",
    "a = remove_small(tile.crf, min_size = 25)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "t = tile.tile\n",
    "t=t-np.min(t)\n",
    "t/=np.max(t)\n",
    "\n",
    "r = [3000, 7000, 0, 4000]\n",
    "#plt.imshow(t, cmap='gray')\n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(tile.labels[r[0]:r[1],r[2]:r[3]])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(tile.preds[r[0]:r[1],r[2]:r[3]])\n",
    "#plt.subplot(1,3,3)\n",
    "#plt.imshow(tile_032026.cleaned_preds[3000:7000,0:4000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.plt_tile(tile.labels[3000:7000,0:4000], tile.preds[3000:7000,0:4000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "        0: ('Reds', 0.0, 'Fundo'),\n",
    "        1: ('Blues', 0.8, 'Loteamento Vazio'),\n",
    "        2: ('Greens', 0.8, 'Outros Equipamentos'),\n",
    "        3: ('Reds', 0.8, 'Vazio Intraurbano'),\n",
    "        4: ('Oranges', 0.8, 'Área Urbanizada')\n",
    "    }\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "view.plot_masked_image(tile.labels[3000:7000,0:4000], label_map, image=None, title=\"Mask Overlay Only\", ax=axs[0])\n",
    "view.plot_masked_image(tile.preds[3000:7000,0:4000], label_map, image=None, title=\"Mask Overlay Only\", ax=axs[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 16))\n",
    "view.plot_masked_image(tile.cleaned_preds[3000:7000,0:4000], label_map, image=None, title=\"Cleaned\", ax=axs[0])\n",
    "view.plot_masked_image(tile.crf[3000:7000,0:4000], label_map, image=None, title=\"CRF\", ax=axs[1])\n",
    "view.plot_masked_image(tile.cleaned_crf[3000:7000,0:4000], label_map, image=None, title=\"CRF+Clean\", ax=axs[2])\n",
    "view.plot_masked_image(tile.vi_completion[3000:7000,0:4000], label_map, image=None, title=\"+completion\", ax=axs[3])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(pca_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(tile_032026.logits)\n",
    "tile_032026.preds[:200,:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(tile_032026.logits_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (256, 256+32)\n",
    "y = (2256, 2256+32)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(tile_032026.logits[x[0]:x[1],y[0]:y[1]])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(tile_032026.logits_0[x[0]:x[1],y[0]:y[1]])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(tile_032026.labels[x[0]:x[1],y[0]:y[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/jonathan/UrbanizedAreasSegmentation/data/results/S2-16D_V2_032027/6x6_subtiles/UNet-256-type-DS-CEW/S2-16D_V2_032027.tif'\n",
    "with rasterio.open(path) as src:\n",
    "    data = src.read()\n",
    "    meta = src.meta\n",
    "plt.imshow(np.squeeze(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
