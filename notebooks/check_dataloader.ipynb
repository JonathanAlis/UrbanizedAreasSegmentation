{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checagem de dados\n",
    "\n",
    "Scripts com exemplos de como fazer o carregamento e divisão dos dados de forma estratificada.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rasterio.coords import BoundingBox\n",
    "\n",
    "import src.data.preprocess_data as data\n",
    "import src.data.view as view\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definições\n",
    "Quais tiles e o número de subtiels são definidos a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = ['032027']#, '032026'] \n",
    "num_subtiles = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisão do conjunto de treino, validação e teste, em que cada arquivo em data/processed dos tiles correspondentes são associados a um dos conjuntos.\n",
    "\n",
    "Primeiramente, sem estratificação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_no_strat, val_files_no_strat, test_files_no_strat = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2)\n",
    "\n",
    "data.count_classes(train_files_no_strat+val_files_no_strat+test_files_no_strat, num_subtiles=6, agregate_by='type')\n",
    "\n",
    "data.count_classes(train_files_no_strat, num_subtiles=6, agregate_by='type')\n",
    "data.count_classes(val_files_no_strat, num_subtiles=6, agregate_by='type')\n",
    "data.count_classes(test_files_no_strat, num_subtiles=6, agregate_by='type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Com estratificação, baseada nas máscaras, de acordo com o valor de classe escolhida pelo usuário.\n",
    "\n",
    "Neste exemplo, é feita pelo tipo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checagem de resultados da estratificação\n",
    "\n",
    "data.check_stratification imprime o número de pixels de cada classe por divisão de conjunto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = 'type')\n",
    "\n",
    "train_files, val_files, test_files\n",
    "print(len(train_files), len(val_files), len(test_files))\n",
    "\n",
    "data.count_classes(train_files+val_files+test_files, num_subtiles=6, agregate_by='type')\n",
    "\n",
    "data.count_classes(train_files, num_subtiles=6, agregate_by='type')\n",
    "data.count_classes(val_files, num_subtiles=6, agregate_by='type')\n",
    "data.count_classes(test_files, num_subtiles=6, agregate_by='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar e carregar\n",
    "\n",
    "Um arquivo yaml é salvo automaticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset e dataloader\n",
    "\n",
    "Carregamento de imagens e mascaras, por tipo.\n",
    "\n",
    "É feita com janelamento com stride de metade do tamanho da imagem.\n",
    "\n",
    "Adicionalmente, é feito data augmentation. As classes minoritárias que tem 1% ou mais de área da imagem geram 8 cópias contendo as transformacoes \"Dihedral Group of Order 8\" (ou D₄)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dataset = data.SubtileDataset(train_files, \n",
    "                    num_subtiles = num_subtiles, \n",
    "                    classes_mode = 'type', \n",
    "                    patch_size=256, \n",
    "                    stride = 256,\n",
    "                    dynamic_sampling=False,\n",
    "                    data_augmentation = False, \n",
    "                    ignore_most_nans = True, # testando \n",
    "                    )\n",
    "\n",
    "class_counter, class_counter_img = train_dataset.count_classes()\n",
    "percentual_counter = [c/sum(class_counter) for c in class_counter]\n",
    "img_percentual = [c/len(train_dataset) for c in class_counter_img]\n",
    "print(class_counter)\n",
    "print(percentual_counter)\n",
    "print(class_counter_img)\n",
    "print(img_percentual)\n",
    "[1/c for c in img_percentual]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dataset = data.SubtileDataset(train_files, \n",
    "                    num_subtiles = num_subtiles, \n",
    "                    classes_mode = 'type', \n",
    "                    patch_size=256, \n",
    "                    stride = 256,\n",
    "                    dynamic_sampling=False,\n",
    "                    data_augmentation = False, \n",
    "                    ignore_most_nans = False, # testando \n",
    "                    )\n",
    "\n",
    "class_counter, class_counter_img = train_dataset.count_classes()\n",
    "percentual_counter = [c/sum(class_counter) for c in class_counter]\n",
    "img_percentual = [c/len(train_dataset) for c in class_counter_img]\n",
    "print(class_counter)\n",
    "print(percentual_counter)\n",
    "print(class_counter_img)\n",
    "print(img_percentual)\n",
    "[1/c for c in img_percentual]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filename = data.yaml_filename(num_subtiles, tiles, stratified_by = 'type')\n",
    "\n",
    "print(yaml_filename)\n",
    "train_dataset = data.SubtileDataset(yaml_filename,\n",
    "                                    set = 'train_files',\n",
    "                                    patch_size=256, \n",
    "                                    stride=256, \n",
    "                                    dynamic_sampling=True,\n",
    "                                    data_augmentation = True, # testando \n",
    "                                    )\n",
    "print(len(train_dataset))\n",
    "class_counter, class_counter_img = train_dataset.count_classes()\n",
    "percentual_counter = [c/sum(class_counter) for c in class_counter]\n",
    "img_percentual = [c/len(train_dataset) for c in class_counter_img]\n",
    "print(class_counter)\n",
    "print(percentual_counter)\n",
    "print(class_counter_img)\n",
    "print(img_percentual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing data augmentation, stage 1...\n",
    "Before data augmentation stage 1:\n",
    "Pixels for each class: tensor([78086449,   119428,   175285,    11951,  1888487])\n",
    "Num images with each class: tensor([1225,   82,   67,    5,  368])\n",
    "[4, 1, 2, 3]\n",
    "100%|██████████| 1225/1225 [08:12<00:00,  2.49it/s]\n",
    "Before data augmentation stage 2:\n",
    "Pixels for each class: tensor([217938265,   1339280,   6290601,   2197203,  38638491])\n",
    "Num images with each class: tensor([4065, 1042, 2195,  965, 2596])\n",
    "[4, 2, 1, 3]\n",
    "[tensor(1), tensor(1), tensor(3), tensor(4)]\n",
    "100%|██████████| 4065/4065 [00:11<00:00, 352.55it/s]\n",
    "After data augmentation:\n",
    "Pixels for each class: tensor([745605717,   3140102,  24110105,   4915387, 111290065])\n",
    "Num images with each class: tensor([13566,  2399,  3515,  2197,  9073])\n",
    "Starting from 1225 images\n",
    "Dinamic Window step added 2840 images\n",
    "Data augmentation added 9501 images with transform\n",
    "Total: 13566\n",
    "tensor([745605717,   3140102,  24110105,   4915387, 111290065])\n",
    "[tensor(0.8386), tensor(0.0035), tensor(0.0271), tensor(0.0055), tensor(0.1252)]\n",
    "tensor([13566,  2399,  3515,  2197,  9073])\n",
    "[tensor(1.), tensor(0.1768), tensor(0.2591), tensor(0.1619), tensor(0.6688)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    train_dataset = data.SubtileDataset(train_files, \n",
    "                    num_subtiles = num_subtiles, \n",
    "                    classes_mode = 'type', \n",
    "                    patch_size=256, \n",
    "                    stride=256, \n",
    "                    data_augmentation = 1, # testando \n",
    "                    return_imgidx = False,\n",
    "                    treat_nans=True, # they supposelly are saved as negative, then a true here cut costs\n",
    "                    debug=False, \n",
    "                    augmentation_thresholds=(0.05, 0.5))\n",
    "\n",
    "    class_counter, class_counter_img = train_dataset.count_classes()\n",
    "    percentual_counter = [c/sum(class_counter) for c in class_counter]\n",
    "    img_percentual = [c/len(train_dataset) for c in class_counter_img]\n",
    "    print(class_counter)\n",
    "    print(percentual_counter)\n",
    "    print(class_counter_img)\n",
    "    print(img_percentual)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das 9800 imagens, 1455 delas tiveram mais de 1% de área de classes minoritárias. Essas são readicionadas no dataset em 7 transformações adicionais.\n",
    "\n",
    "Isso gera, para este conjunto de treino, 1455*7 + 9800 = 19985 imagens.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "for img, mask in train_dataset:\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 0\n",
    "for img, mask in train_dataset:\n",
    "    if  (mask != 0).any():\n",
    "        #subtile_composition.display_images(img)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(mask.squeeze())\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(img[2].squeeze(),cmap='gray')\n",
    "        plt.show()\n",
    "        i+=1\n",
    "\n",
    "    if i == 3:\n",
    "        pass\n",
    "        break\n",
    "    print(i)\n",
    "    #img,mask = next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(train_dataset)):\n",
    "    image, label = train_dataset[i]\n",
    "    print(f\"Sample {i}: Image shape = {image.shape}, Label shape = {label.shape}\")\n",
    "#4 min, with nan =  false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Para carregar várias imagens em batches, usamos dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=16, \n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    img = batch[0]\n",
    "    mask = batch[1]\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    print(mask.unique(return_counts=True))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA components\n",
    "working_dir = os.path.abspath('..')\n",
    "save_path = os.path.join(working_dir, 'config', 'pca_weights.npy')\n",
    "pca_weights = data.compute_pca_from_dataloader(train_loader, save_path=save_path)\n",
    "print(\"PCA Weights Shape:\", pca_weights.shape)  # Expected: [3, 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,lab in train_loader:\n",
    "    print(pca_weights.shape)\n",
    "    print(img.shape)\n",
    "    pca_img = data.apply_pca_weights(img, torch.Tensor(pca_weights.T))\n",
    "    #plt.imshow(img)\n",
    "    print(pca_img.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "view.plot_pca_batch(pca_img, images_per_row=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrando 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtile_composition.display_images(img)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(img[i,1,:,:].squeeze(),cmap='gray')\n",
    "    plt.imshow(mask[i,...])\n",
    "    plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    img = batch[0]\n",
    "    mask = batch[1]\n",
    "    unique_classes, counts = torch.unique(mask, return_counts=True)\n",
    "    print(\"Class distribution in y_true:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Class {cls.item()}: {count.item()} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    image, label = train_dataset[i]\n",
    "    print(f\"Sample {i}: Image shape = {image.shape}, Label shape = {label.shape}\")\n",
    "    \n",
    "    unique_classes, counts = torch.unique(label, return_counts=True)\n",
    "    print(\"Class distribution in y_true:\")\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Class {cls.item()}: {count.item()} samples\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando a interpolação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath('..')\n",
    "train_dataset_nans = data.SubtileDataset(train_files+val_files+test_files, \n",
    "                    num_subtiles = num_subtiles, \n",
    "                    classes_mode = 'type', \n",
    "                    patch_size=256, \n",
    "                    stride=256, # sem overlap \n",
    "                    data_augmentation = False, \n",
    "                    ignore_most_nans= False,\n",
    "                    return_imgidx = False,\n",
    "                    treat_nans=False, # they supposelly are saved as negative, then a true here cut costs\n",
    "                    debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "most_nan_img = None\n",
    "most_nan_mask = None\n",
    "num_nan = 0\n",
    "\n",
    "indices = []\n",
    "num_nans = []\n",
    "for i, (img, mask) in enumerate(train_dataset_nans):\n",
    "    negatives = torch.sum(img < 0).item()\n",
    "    urban = torch.sum(mask > 0).item()\n",
    "    #if negatives > 0 and urban > 0:\n",
    "    indices.append(i)\n",
    "    num_nans.append(negatives)\n",
    "\n",
    "ordered_idx = np.argsort(num_nans)\n",
    "len(ordered_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(num_nans)\n",
    "print(indices)\n",
    "print([indices[i] for i in ordered_idx])\n",
    "print([num_nans[i] for i in ordered_idx])\n",
    "print([num_nans[i]/img.numel() for i in ordered_idx])\n",
    "\n",
    "data = [num_nans[i]/img.numel()*100 for i in ordered_idx]\n",
    "plt.figure(figsize=(14, 8))\n",
    "counts, bins, patches = plt.hist(data, bins=30, edgecolor='black', weights=[100/len(data)]*len(data))  # Histogram\n",
    "# Calculate midpoints for labels\n",
    "midpoints = 0.5 * (bins[1:] + bins[:-1])  # Midpoints for 20 bins\n",
    "bin_labels = [f'{bins[i]:.2f}-{bins[i+1]:.2f}' for i in range(len(bins)-1)]\n",
    "\n",
    "for count, bin_edge in zip(counts, bins[:-1]):\n",
    "    plt.text(bin_edge + (bins[1] - bins[0]) / 2, count, f'{count:.2f}%', ha='center', va='bottom', rotation=45)  # Centered above each bar\n",
    "# Apply labels to midpoints\n",
    "plt.xticks(midpoints, bin_labels, rotation=45)  # Use midpoints instead of bin edges\n",
    "\n",
    "plt.xlabel('Percentual de NaN na imagem')\n",
    "plt.ylabel('Quantidade de imagens')\n",
    "plt.title('Distribuição de quantidade de NaN em dataset de 9800 imagens')\n",
    "#plt.yscale('log')  # Use log scale to better visualize large ranges\n",
    "\n",
    "save_to = os.path.join(working_dir, 'figs', 'nans_distribution.png')\n",
    "plt.savefig(save_to, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nans.sort\n",
    "most_nan_img = img.clone() \n",
    "num_nan = negatives\n",
    "most_nan_mask = mask.clone()\n",
    "#print(type(most_nan_img))\n",
    "#print(img.shape)\n",
    "num_nan\n",
    "choices = [int(ordered_idx[j]) for j in [9500, 9650, 9800-1]]\n",
    "#choices = [indices[i] for i in [j for j in ordered_idx[1000, 1200, 1343]]]\n",
    "\n",
    "print(choices)\n",
    "print([num_nans[i] for i in choices])\n",
    "imgs = []\n",
    "masks = []\n",
    "for c in choices:\n",
    "    img,mask = train_dataset_nans[c]\n",
    "    imgs.append(img)\n",
    "    masks.append(mask)\n",
    "    print(torch.sum(img < 0).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nan_value = most_nan_img.max()\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "\n",
    "for idx in range(3):\n",
    "    plt.subplot(1,3,idx+1)\n",
    "    img_ = imgs[idx]\n",
    "    mask_ = masks[idx]\n",
    "    r = img_[1,:,:].squeeze()\n",
    "    g = img_[2,:,:].squeeze()\n",
    "    b = img_[3,:,:].squeeze()\n",
    "    rgb_image = torch.stack([r, g, b], axis=-1)\n",
    "    min = rgb_image[rgb_image > 0].min().item()\n",
    "    rgb_image = rgb_image-min\n",
    "    rgb_image*=1/(rgb_image.max())\n",
    "    for i in range(r.shape[0]):\n",
    "        for j in range(r.shape[1]):\n",
    "            if r[i,j]<0:\n",
    "                rgb_image[i,j,0] = 1\n",
    "                rgb_image[i,j,1] = 0\n",
    "                rgb_image[i,j,2] = 0\n",
    "    #rgb_image = rgb_image-rgb_image.min()\n",
    "    #rgb_image*=1/(rgb_image.max())\n",
    "    \n",
    "    plt.imshow(rgb_image)\n",
    "    #plt.imshow(mask_.squeeze(), alpha=0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.axis('off')\n",
    "    \n",
    "\n",
    "save_to = os.path.join(working_dir, 'figs', 'invalid_pixels.png')\n",
    "plt.savefig(save_to, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nan_value = most_nan_img.max()\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "\n",
    "for idx in range(3):\n",
    "    plt.subplot(1,3,idx+1)\n",
    "    img_ = torch.abs(imgs[idx])\n",
    "    mask_ = masks[idx]\n",
    "    r = img_[1,:,:].squeeze()\n",
    "    g = img_[2,:,:].squeeze()\n",
    "    b = img_[3,:,:].squeeze()\n",
    "    rgb_image = torch.stack([r, g, b], axis=-1)\n",
    "    min = rgb_image[rgb_image > 0].min().item()\n",
    "    rgb_image = rgb_image-min\n",
    "    rgb_image*=1/(rgb_image.max())\n",
    "    for i in range(r.shape[0]):\n",
    "        for j in range(r.shape[1]):\n",
    "            if r[i,j]<0:\n",
    "                rgb_image[i,j,0] = 1\n",
    "                rgb_image[i,j,1] = 0\n",
    "                rgb_image[i,j,2] = 0\n",
    "    #rgb_image = rgb_image-rgb_image.min()\n",
    "    #rgb_image*=1/(rgb_image.max())\n",
    "    \n",
    "    plt.imshow(rgb_image)\n",
    "    #plt.imshow(mask_.squeeze(), alpha=0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.axis('off')\n",
    "    \n",
    "save_to = os.path.join(working_dir, 'figs', 'nearest_pixels.png')\n",
    "plt.savefig(save_to, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, test_files\n",
    "len(train_files), len(val_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "working_dir = os.path.abspath('..')\n",
    "\n",
    "def fill_with(f, rgb = (0,0,1)):\n",
    "    x, y = data.extract_integers(f)\n",
    "    x = x//ratio\n",
    "    y = y//ratio\n",
    "    set_division[x+borda:x+subtile_size-borda, y+borda:y+subtile_size-borda, 0] = rgb[0]\n",
    "    set_division[x+borda:x+subtile_size-borda, y+borda:y+subtile_size-borda, 1] = rgb[1]\n",
    "    set_division[x+borda:x+subtile_size-borda, y+borda:y+subtile_size-borda, 2] = rgb[2]\n",
    "ratio = 10\n",
    "set_division = np.zeros(shape=(10560//ratio,10560//ratio, 3))\n",
    "subtile_size = 10560//6//ratio\n",
    "borda = 3\n",
    "\n",
    "for f in train_files:\n",
    "    fill_with(f, rgb = (1,0,0))\n",
    "for f in val_files:\n",
    "    fill_with(f, rgb = (0,1,0))\n",
    "for f in test_files:\n",
    "    fill_with(f, rgb = (0,0,1))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(set_division)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "    \n",
    "save_to = os.path.join(working_dir, 'figs', 'set_division.png')\n",
    "plt.savefig(save_to, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.7, \n",
    "                                                                    val_size = 0.15, \n",
    "                                                                    stratify_by = 'type',\n",
    "                                                                    debug = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
