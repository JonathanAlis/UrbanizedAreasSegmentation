{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência\n",
    "\n",
    "É aplicação do modelo nos patches do conjunto de teste, e avaliação das métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports e preparação do device CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import src.data.preprocess_data as data\n",
    "import src.training.train_model as train\n",
    "import src.data.view as view\n",
    "import src.models.unets as unets\n",
    "import src.models.hrnets as hrnets\n",
    "import src.training.post_processing as post\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def print_gpu_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"{prefix} Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"{prefix} Memory Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição nos conjuntos de teste\n",
    "\n",
    "Para cada um dos conjuntos de tiles, há a divisão entre treino, validaçao e teste, a inferência utiliza o conjunto de teste de cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_1 = {\n",
    "              'Belo Horizonte': '032027',\n",
    "              }\n",
    "\n",
    "tiles_4 = {\n",
    "              'Manaus': '016009',\n",
    "              'Porto Alegre': '025037',\n",
    "              'Belo Horizonte': '032027',\n",
    "              'Salvador': '038019',      \n",
    "              }\n",
    "tiles_8 = {\n",
    "              'Boa Vista': '015002',  \n",
    "              'Campo Grande': '021027',\n",
    "              'Macapá': '025005',\n",
    "              'Curitiba': '027032',\n",
    "              'Brasília': '028022',                      \n",
    "              'Rio de Janeiro': '033029',\n",
    "              'Teresina': '034011',\n",
    "              'Petrolina': '036016',\n",
    "              }\n",
    "\n",
    "tiles = {}\n",
    "tiles['1 tile'] = list(tiles_1.values())\n",
    "tiles['4 tiles'] = list(tiles_4.values())\n",
    "tiles['8 tiles'] = list(tiles_8.values())\n",
    "tiles['12 tiles'] = list(set(list(tiles_4.values())+list(tiles_8.values())))\n",
    "\n",
    "\n",
    "num_subtiles = 6\n",
    "classes_mode = '4types'\n",
    "training_batch_size = 16\n",
    "model_types = 'unets'\n",
    "weighted = True\n",
    "\n",
    "if classes_mode == 'type':\n",
    "    num_classes = 5\n",
    "elif classes_mode == 'density':\n",
    "    num_classes = 4\n",
    "elif classes_mode == '4types':\n",
    "    num_classes = 4\n",
    "elif classes_mode == 'binary':\n",
    "    num_classes = 2\n",
    "elif classes_mode == 'all':\n",
    "    num_classes = 9\n",
    "\n",
    "\n",
    "working_dir = os.path.abspath('..')\n",
    "models_paths = os.listdir(os.path.join(working_dir, 'models'))\n",
    "models_paths = [f for f in models_paths if f.endswith('4tt.pth')]\n",
    "models_paths = [f for f in models_paths if f.startswith('UNetSmall')]\n",
    "models_paths.sort()\n",
    "\n",
    "\n",
    "channels_dict = {}\n",
    "channels_dict[12] = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A']\n",
    "#channels_dict[10] = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']\n",
    "channels_dict[8] = ['B02', 'B03', 'B04', 'B05', 'B06', 'B08', 'B11', 'B12']\n",
    "#channels_dict[6] = ['B02', 'B03', 'B04', 'B06', 'B08', 'B11']\n",
    "channels_dict[4] = ['B02', 'B03', 'B04','B08']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos escolhidos:\n",
    "\n",
    "UNetSmall, emm configuração DS-CEW, para 8 e 4 canais, tanto modelo original como finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath('..')\n",
    "models_paths = os.listdir(os.path.join(working_dir, 'models'))\n",
    "models_paths += os.listdir(os.path.join(working_dir, 'models', 'finetuned'))\n",
    "#models_paths = [f for f in models_paths if (f.endswith('8ft.pth') or f.endswith('4tt.pth'))]\n",
    "models_paths = [f for f in models_paths if f.startswith('UNetSmall')]\n",
    "models_paths = [f for f in models_paths if 'DS-CEW' in f]\n",
    "models_paths = [f for f in models_paths if ('-8ch-' in f or '-4ch-' in f)]\n",
    "\n",
    "models_paths.sort()\n",
    "models_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.DataFrame(columns = ['tile','model', 'metric', 'macro avg', 'weighted avg', 'Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "metrics.set_index([\"tile\", \"model\", \"metric\"], inplace=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição nos conjuntos de teste\n",
    "\n",
    "Para cada um dos conjuntos de tiles, há a divisão entre treino, validaçao e teste, a inferência utiliza o conjunto de teste de cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tile_set in tiles:\n",
    "    \n",
    "    train_files, val_files, test_files = data.train_val_test_stratify(tiles[tile_set], \n",
    "                                                                    num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = classes_mode,\n",
    "                                                                    subfolder='q_12ch')\n",
    "\n",
    "    class_labels = list(range(num_classes))\n",
    "\n",
    "    for model_name in models_paths:# if 'UNet-256-type-DS-CEW' in mp]:#model_paths:\n",
    "    #for model_name in model_paths:#[mp for mp in models_paths if 'UNet-256-type-DS-CEW' in mp]:#model_paths:\n",
    "        print('Model name:', model_name)\n",
    "        #UNet-256-4types-DS-CE-12ch-4tt.pth\n",
    "        import re\n",
    "        match = re.search(r\"(\\d+)ch\", model_name)\n",
    "        if match:\n",
    "            in_channels = int(match.group(1))  # Saída: 12\n",
    "            print('Input channels: ', in_channels)\n",
    "        print('Num classes:', num_classes)\n",
    "        if model_name.startswith('UNetSmall-'):\n",
    "            model = unets.UNetSmall(in_channels=in_channels, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNet-'):\n",
    "            model = unets.UNet(in_channels=in_channels, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetResNet34-'):\n",
    "            model = unets.UNetResNet34(in_channels=in_channels, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "            model = unets.UNetEfficientNetB0(in_channels=in_channels, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetConvNext-'):\n",
    "            model = unets.UNetConvNext(in_channels=in_channels, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('HRNetW18'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= in_channels, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "        elif model_name.startswith('HRNetW32'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= in_channels, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "        elif model_name.startswith('HRNetW48'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= in_channels, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "        else:\n",
    "            print('Nao existe esse modelo')\n",
    "            continue\n",
    "        if 'finetuned' in model_name:\n",
    "            checkpoint = torch.load(os.path.join(working_dir, 'models', 'finetuned', model_name), weights_only=False)\n",
    "        else:\n",
    "            checkpoint = torch.load(os.path.join(working_dir, 'models', model_name), weights_only=False)\n",
    "\n",
    "\n",
    "        \n",
    "        indices = [i for i, value in enumerate(channels_dict[in_channels]) if value in channels_dict[12]]\n",
    "\n",
    "        yaml_filename = data.yaml_filename(num_subtiles, tiles[tile_set], classes_mode)\n",
    "        print(yaml_filename)\n",
    "        print('----------------')\n",
    "        patch_size = int(model_name.split('-')[1])\n",
    "        \n",
    "        test_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                        set = 'test_files',\n",
    "                                        patch_size=patch_size, \n",
    "                                        stride=patch_size, \n",
    "                                        dynamic_sampling = False,\n",
    "                                        data_augmentation = False, # testando \n",
    "                                        channels_subset = indices\n",
    "                                        )\n",
    "        BS = 16\n",
    "        dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False)\n",
    "\n",
    "        print(model_name)\n",
    "        recalls, report = train.test_model(model, \n",
    "                        checkpoint_path=model_name,\n",
    "                        dataloader = dataloader, \n",
    "                        device = device, \n",
    "                        num_classes = num_classes,\n",
    "                        subfolder= 'finetuned' if 'finetuned' in model_name else '',\n",
    "                        set_name = f'-test-{tile_set}'\n",
    "                        ) \n",
    "\n",
    "        metrics.loc[(tile_set, model_name, \"f1-score\"), 'macro avg'] = report['f1-score']['macro avg']\n",
    "        metrics.loc[(tile_set, model_name, \"f1-score\"), 'weighted avg'] = report['f1-score']['weighted avg']\n",
    "        for i, cl in enumerate(class_labels):\n",
    "            metrics.loc[(tile_set, model_name, \"f1-score\"), f'Class {i}'] = report['f1-score'][f'Class {i}']\n",
    "\n",
    "        metrics.loc[(tile_set, model_name, \"precision\"), 'macro avg'] = report['precision']['macro avg']\n",
    "        metrics.loc[(tile_set, model_name, \"precision\"), 'weighted avg'] = report['precision']['weighted avg']\n",
    "        for i, cl in enumerate(class_labels):\n",
    "            metrics.loc[(tile_set, model_name, \"precision\"), f'Class {i}'] = report['precision'][f'Class {i}']\n",
    "\n",
    "        metrics.loc[(tile_set, model_name, \"recall\"), 'macro avg'] = report['recall']['macro avg']\n",
    "        metrics.loc[(tile_set, model_name, \"recall\"), 'weighted avg'] = report['recall']['weighted avg']\n",
    "        for i, cl in enumerate(class_labels):\n",
    "            metrics.loc[(tile_set, model_name, \"recall\"), f'Class {i}'] = report['recall'][f'Class {i}']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização dos resultados\n",
    "\n",
    "metrics contém a tabela de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_percent = metrics.applymap(lambda x: f\"{x * 100:.1f}\")\n",
    "metrics_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_percent.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tile_set in tiles:\n",
    "    print(tiles[tile_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
