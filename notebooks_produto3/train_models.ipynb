{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento\n",
    "\n",
    "Carrega os imports, define os tiles, define os parametros de dados e de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"{prefix} Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"{prefix} Memory Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import src.models.unets as unets\n",
    "import src.data.preprocess_data as data\n",
    "import src.training.train_model as train\n",
    "import src.models.hrnets as hrnets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tiles_1 = {\n",
    "              'Belo Horizonte': '032027',\n",
    "              }\n",
    "\n",
    "tiles_4 = {\n",
    "              'Manaus': '016009',\n",
    "              'Porto Alegre': '025037',\n",
    "              'Belo Horizonte': '032027',\n",
    "              'Salvador': '038019',      \n",
    "              }\n",
    "tiles_8 = {\n",
    "              'Boa Vista': '015002',  \n",
    "              'Campo Grande': '021027',\n",
    "              'Macapá': '025005',\n",
    "              'Curitiba': '027032',\n",
    "              'Brasília': '028022',                      \n",
    "              'Rio de Janeiro': '033029',\n",
    "              'Teresina': '034011',\n",
    "              'Petrolina': '036016',\n",
    "              }\n",
    "\n",
    "tiles = {}\n",
    "tiles['1 tile'] = list(tiles_1.values())\n",
    "tiles['4 tiles'] = list(tiles_4.values())\n",
    "tiles['8 tiles'] = list(tiles_8.values())\n",
    "tiles['12 tiles'] = list(set(list(tiles_4.values())+list(tiles_8.values())))\n",
    "\n",
    "\n",
    "\n",
    "tiles = tiles['8 tiles'] \n",
    "num_subtiles = 6\n",
    "classes_mode = '4types'\n",
    "model_types = 'unets'\n",
    "\n",
    "if model_types=='unets':\n",
    "    training_batch_size = 16\n",
    "\n",
    "#model_types = 'unets'\n",
    "\n",
    "if classes_mode == 'type':\n",
    "    num_classes = 5\n",
    "elif classes_mode == '4types':\n",
    "    num_classes = 4\n",
    "elif classes_mode == 'density':\n",
    "    num_classes = 4\n",
    "elif classes_mode == 'binary':\n",
    "    num_classes = 2\n",
    "elif classes_mode == 'all':\n",
    "    num_classes = 9\n",
    "\n",
    "\n",
    "\n",
    "channels_dict = {}\n",
    "channels_dict[12] = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A']\n",
    "channels_dict[10] = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']\n",
    "channels_dict[8] = ['B02', 'B03', 'B04', 'B05', 'B06', 'B08', 'B11', 'B12']\n",
    "channels_dict[6] = ['B02', 'B03', 'B04', 'B06', 'B08', 'B11']\n",
    "channels_dict[4] = ['B02', 'B03', 'B04','B08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_models_batch = {f'UNetSmall-64-{classes_mode}' : 16, #512,\n",
    "                     f'UNetSmall-256-{classes_mode}' : 32,\n",
    "                     f'UNet-64-{classes_mode}': 256,\n",
    "                     f'UNet-256-{classes_mode}': 16,\n",
    "                     f'UNetResNet34-224-{classes_mode}': 128, #ok\n",
    "                     f'UNetEfficientNetB0-224-{classes_mode}': 64, \n",
    "                     f'UNetConvNext-224-{classes_mode}': 32,\n",
    "                     f'HRNetW18-512-{classes_mode}': 4,\n",
    "                     f'HRNetW32-512-{classes_mode}': 4,\n",
    "                     f'HRNetW48-512-{classes_mode}': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_types == 'unets':\n",
    "    model_param_grid = {\n",
    "\n",
    "        #model params:\n",
    "        \n",
    "        'model' : [#f'UNetSmall-64-{classes_mode}',\n",
    "                   f'UNetSmall-256-{classes_mode}',\n",
    "               #f'UNet-256-{classes_mode}', #ok\n",
    "                #f'UNet-64-{classes_mode}', #ok\n",
    "                #f'UNetResNet34-224-{classes_mode}', #ok\n",
    "                #f'UNetEfficientNetB0-224-{classes_mode}', \n",
    "                #f'UNetConvNext-224-{classes_mode}',\n",
    "                ],\n",
    "            \n",
    "        #training params\n",
    "            # loss\n",
    "        'loss': ['CEW'],#macroF2', 'macroF2W'],#['CE', 'CEW'], #-dice', 'dice'],#,'groups'],#, 'dice', 'CE-dice'],\n",
    "        'epochs' : [15],\n",
    "        'patience' : [3],\n",
    "    }\n",
    "data_param_grid = {\n",
    "        'batch_size' : [training_batch_size],\n",
    "        'dynamic_sampling' : [True],\n",
    "        'data_augmentation' : [False],\n",
    "        'num_channels': [8, 4],#[12, 10, 8, 6, 4],\n",
    "        'patch_size': [256]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = classes_mode,\n",
    "                                                                    subfolder='q_12ch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treino\n",
    "\n",
    "Em 2 loops: sobre os parametros dos dados, e sobre os parametros dos modelos\n",
    "\n",
    "Varia sobre todas as combinações de parametros dos dois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_params in train.iterate_parameter_grid(data_param_grid):\n",
    "\n",
    "    num_ch = data_params['num_channels']\n",
    "    indices = [i for i, value in enumerate(channels_dict[num_ch]) if value in channels_dict[12]]\n",
    "\n",
    "    yaml_filename = data.yaml_filename(num_subtiles, tiles, classes_mode)\n",
    "    \n",
    "    #print('Trying to read from:', yaml_filename)\n",
    "    train_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'train_files',\n",
    "                                    patch_size=data_params['patch_size'], \n",
    "                                    stride=data_params['patch_size'],\n",
    "                                    classes_mode=classes_mode,\n",
    "                                    channels_subset= indices,\n",
    "                                    dynamic_sampling = data_params['dynamic_sampling'] ,\n",
    "                                    data_augmentation = data_params['data_augmentation'], # testando \n",
    "                                    )\n",
    "    \n",
    "    val_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'val_files',\n",
    "                                    patch_size=data_params['patch_size'], \n",
    "                                    stride=data_params['patch_size'],\n",
    "                                    classes_mode=classes_mode,\n",
    "                                    channels_subset = indices,\n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    )\n",
    "    \n",
    "    test_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'test_files',\n",
    "                                    patch_size=data_params['patch_size'], \n",
    "                                    stride=data_params['patch_size'],\n",
    "                                    classes_mode=classes_mode,\n",
    "                                    channels_subset = indices,\n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    )\n",
    "\n",
    "    for model_params in train.iterate_parameter_grid(model_param_grid):\n",
    "\n",
    "        #definitions\n",
    "        #if classes_mode == 'binary' and model_params['weighted_loss']: #duas classes, sem weighted loss\n",
    "        #    continue\n",
    "\n",
    "        \n",
    "        model_name = model_params['model']\n",
    "        training_batch_size = min(16, unet_models_batch[model_name])\n",
    "        if data_params['dynamic_sampling']:\n",
    "            model_name+='-DS'\n",
    "        if data_params['data_augmentation']:\n",
    "            model_name+='-DA'\n",
    "        model_name += f'-{model_params[\"loss\"]}'\n",
    "        model_name+=f\"-{data_params['num_channels']}ch\" #nof channels\n",
    "        model_name+=f\"-{len(tiles)}tt\" #nof train tiles\n",
    "            \n",
    "        weighted_loss = model_params[\"loss\"].endswith('W')\n",
    "        \n",
    "        patch_size = int(model_name.split('-')[1])\n",
    "        print('--------------------')\n",
    "        print('Training', model_name)\n",
    "        print(model_params)\n",
    "        model_class = model_name.split('-')[0]\n",
    "        patch_size = int(model_name.split('-')[1])\n",
    "\n",
    "        if not weighted_loss and not data_params['dynamic_sampling']:\n",
    "            continue #ignore CE\n",
    "        #load data\n",
    "\n",
    "        # define quais indices\n",
    "\n",
    "        if weighted_loss:                   \n",
    "            class_counts, per_image = train_dataset.count_classes()\n",
    "            class_weights = 1.0 / class_counts  # Inverse of class frequencies\n",
    "            class_weights = class_weights / torch.sum(class_weights)  # Normalize\n",
    "        else:    \n",
    "            class_weights = None\n",
    "\n",
    "        dynamic_sampling = train_dataset.dynamic_sampling\n",
    "        data_augmentation = train_dataset.data_augmentation   \n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=training_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=training_batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=training_batch_size, shuffle=False)\n",
    "\n",
    "        if model_name.startswith('UNetSmall-'):\n",
    "            model = unets.UNetSmall(in_channels=num_ch, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNet-'):\n",
    "            model = unets.UNet(in_channels=num_ch, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetResNet34-'):\n",
    "            model = unets.UNetResNet34(in_channels=num_ch, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "            model = unets.UNetEfficientNetB0(in_channels=num_ch, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('UNetConvNext-'):\n",
    "            model = unets.UNetConvNext (in_channels=num_ch, out_channels=num_classes).to(device) \n",
    "        elif model_name.startswith('HRNetW18'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= num_ch, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "        elif model_name.startswith('HRNetW32'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= num_ch, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "        elif model_name.startswith('HRNetW48'):\n",
    "            model = hrnets.HRNetSegmentation(in_channels= num_ch, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "        else:\n",
    "            print(f'Modelo {model_name} não está no param grid. Pulando...')\n",
    "            continue\n",
    "\n",
    "\n",
    "        print(model_params['loss'])\n",
    "        train.train_model(model, \n",
    "                            train_loader, \n",
    "                            val_loader, \n",
    "                            epochs=model_params['epochs'], \n",
    "                            loss_mode = model_params['loss'].replace('W',''),\n",
    "                            device = device,\n",
    "                            num_classes = num_classes, \n",
    "                            simulated_batch_size = training_batch_size, #model_params['batch_size'] ,\n",
    "                            patience = model_params['patience'],\n",
    "                            weights = class_weights,\n",
    "                            show_batches = 1, \n",
    "                            save_to = model_name+'.pth')\n",
    "        if 0:\n",
    "            try:\n",
    "                train.test_model(model, \n",
    "                            checkpoint_path=model_name+'.pth',\n",
    "                            dataloader = test_loader, \n",
    "                            device = device, \n",
    "                            num_classes = num_classes\n",
    "                            ) \n",
    "                            #loss_mode = model_params['loss'], \n",
    "                            #simulated_batch_size = model_params['batch_size'] ,\n",
    "                            #show_batches = 3,\n",
    "                            #yield_predictions = True)\n",
    "            except:\n",
    "                print('ERROR IN TESTING')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim do treinamento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
