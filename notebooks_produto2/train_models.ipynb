{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"{prefix} Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"{prefix} Memory Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import src.models.unets as unets\n",
    "import src.data.preprocess_data as data\n",
    "import src.training.train_model as train\n",
    "import src.models.hrnets as hrnets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definições:\n",
    "\n",
    "defini quais tiles, divisão de subtiles, tipo de modelos e tipos de classes são alvo do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tiles = ['032027']#, '032026'] \n",
    "num_subtiles = 6\n",
    "classes_mode = 'type'\n",
    "model_types = 'unets'\n",
    "\n",
    "if model_types=='hrnets':\n",
    "    training_batch_size = 4\n",
    "if model_types=='unets':\n",
    "    training_batch_size = 16\n",
    "\n",
    "#model_types = 'unets'\n",
    "\n",
    "if classes_mode == 'type':\n",
    "    num_classes = 5\n",
    "elif classes_mode == 'density':\n",
    "    num_classes = 4\n",
    "elif classes_mode == 'binary':\n",
    "    num_classes = 2\n",
    "elif classes_mode == 'all':\n",
    "    num_classes = 9\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos o batch size máximo para cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_models_batch = {f'UNetSmall-64-{classes_mode}' : 16, #512,\n",
    "                     f'UNetSmall-256-{classes_mode}' : 32,\n",
    "                     f'UNet-64-{classes_mode}': 256,\n",
    "                     f'UNet-256-{classes_mode}': 16,\n",
    "                     f'UNetResNet34-224-{classes_mode}': 128, #ok\n",
    "                     f'UNetEfficientNetB0-224-{classes_mode}': 64, \n",
    "                     f'UNetConvNext-224-{classes_mode}': 32,\n",
    "                     f'HRNetW18-512-{classes_mode}': 4,\n",
    "                     f'HRNetW32-512-{classes_mode}': 4,\n",
    "                     f'HRNetW48-512-{classes_mode}': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade de parametros\n",
    "\n",
    "Muitos variações dos modelos vão ser treinados. Aqui definimos quais variações a considerar, entre elas, modelos, tipo de loss, se utilizar ponderação, amostragem dinâmica, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_types == 'unets':\n",
    "    model_param_grid = {\n",
    "\n",
    "        #model params:\n",
    "        \n",
    "        'model' : [f'UNetSmall-64-{classes_mode}',\n",
    "                   f'UNetSmall-256-{classes_mode}',\n",
    "               f'UNet-256-{classes_mode}', #ok\n",
    "                f'UNet-64-{classes_mode}', #ok\n",
    "                f'UNetResNet34-224-{classes_mode}', #ok\n",
    "                f'UNetEfficientNetB0-224-{classes_mode}', \n",
    "                f'UNetConvNext-224-{classes_mode}',\n",
    "                ],\n",
    "            \n",
    "        #training params\n",
    "            # loss\n",
    "        'loss': ['CE'], #-dice', 'dice'],#,'groups'],#, 'dice', 'CE-dice'],\n",
    "        'weighted_loss': [False, True], #Wegted loss, +CE: bom recall pra 2, 3, 4, ruim resto\n",
    "        'dist_loss':[False],\n",
    "        'crf': [False],#[0.0001],    \n",
    "        'epochs' : [15],\n",
    "        'patience' : [3],\n",
    "        'batch_size' : [training_batch_size],\n",
    "        'dynamic_sampling' : [False, True],\n",
    "        'data_augmentation' : [False],\n",
    "        \n",
    "    }\n",
    "\n",
    "if model_types == 'hrnets':\n",
    "    model_param_grid = {\n",
    "        #model params:\n",
    "        'model' : [\n",
    "                #f'HRNetW18-1024-{classes_mode}',\n",
    "                #f'HRNetW32-1024-{classes_mode}',\n",
    "                #f'HRNetW48-1024-{classes_mode}'\n",
    "                f'HRNetW18-512-{classes_mode}',\n",
    "                f'HRNetW32-512-{classes_mode}',\n",
    "                f'HRNetW48-512-{classes_mode}'\n",
    "                ],\n",
    "            \n",
    "        #training params\n",
    "            # loss\n",
    "        'loss': ['CE'], #-dice', 'dice'],#,'groups'],#, 'dice', 'CE-dice'],\n",
    "        'weighted_loss': [False, True], #Wegted loss, +CE: bom recall pra 2, 3, 4, ruim resto\n",
    "        'dist_loss':[False],\n",
    "        'crf': [False],#[0.0001],    \n",
    "        'epochs' : [15],\n",
    "        'patience' : [3],\n",
    "        'batch_size' : [training_batch_size],\n",
    "        'dynamic_sampling' : [False, True],\n",
    "        'data_augmentation' : [False],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação em treino validação e teste, com estratificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files, val_files, test_files = data.train_val_test_stratify(tiles, \n",
    "                                                                  num_subtiles,\n",
    "                                                                    train_size = 0.6, \n",
    "                                                                    val_size = 0.2, \n",
    "                                                                    stratify_by = classes_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treino:\n",
    "\n",
    "Varre o grade de parâmetros, carrega o dataset correspondente, instancia o modelo e treina.\n",
    "\n",
    "Os modelos ficam salvos em models\n",
    "\n",
    "Os resultados e métricas de treino ficam salvos em experimental_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_params in train.iterate_parameter_grid(model_param_grid):\n",
    "\n",
    "    \n",
    "    model_name = model_params['model']\n",
    "    training_batch_size = min(16, unet_models_batch[model_name])\n",
    "    if model_params['crf']:\n",
    "        model_name+='-crf'\n",
    "    if model_params['dist_loss']:\n",
    "        model_name+='-dist'\n",
    "    if model_params['dynamic_sampling']:\n",
    "        model_name+='-DS'\n",
    "    if model_params['data_augmentation']:\n",
    "        model_name+='-DA'\n",
    "    model_name += f'-{model_params[\"loss\"]}'\n",
    "    if model_params['weighted_loss']:\n",
    "        model_name+='W'\n",
    "    patch_size = int(model_name.split('-')[1])\n",
    "    print('--------------------')\n",
    "    print('Training', model_name)\n",
    "    print(model_params)\n",
    "    model_class = model_name.split('-')[0]\n",
    "    patch_size = int(model_name.split('-')[1])\n",
    "\n",
    "    if 0:\n",
    "        if model_params['weighted_loss'] and (model_params['data_augmentation'] or model_params['dynamic_sampling']):\n",
    "            print('Weighted loss: True and some type of data augmentation. It is setup to disconsider this combination.')\n",
    "            print('skipping...')\n",
    "            continue\n",
    "    #load data\n",
    "\n",
    "    yaml_filename = data.yaml_filename(num_subtiles, tiles, classes_mode)\n",
    "    train_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'train_files',\n",
    "                                    patch_size=patch_size, \n",
    "                                    stride=patch_size, \n",
    "                                    dynamic_sampling = model_params['dynamic_sampling'] ,\n",
    "                                    data_augmentation = model_params['data_augmentation'], # testando \n",
    "                                    )\n",
    "    \n",
    "    val_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'val_files',\n",
    "                                    patch_size=patch_size, \n",
    "                                    stride=patch_size, \n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    )\n",
    "    \n",
    "    test_dataset = data.SubtileDataset(yaml_filename, \n",
    "                                    set = 'test_files',\n",
    "                                    patch_size=patch_size, \n",
    "                                    stride=patch_size, \n",
    "                                    dynamic_sampling = False,\n",
    "                                    data_augmentation = False, # testando \n",
    "                                    )\n",
    "\n",
    "    if model_params['weighted_loss']:                   \n",
    "        class_counts, per_image = train_dataset.count_classes()\n",
    "        class_weights = 1.0 / class_counts  # Inverse of class frequencies\n",
    "        class_weights = class_weights / torch.sum(class_weights)  # Normalize\n",
    "    else:    \n",
    "        class_weights = None\n",
    "\n",
    "    dynamic_sampling = train_dataset.dynamic_sampling\n",
    "    data_augmentation = train_dataset.data_augmentation   \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=training_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=training_batch_size, shuffle=False)\n",
    "\n",
    "    if model_name.startswith('UNetSmall-'):\n",
    "        model = unets.UNetSmall(in_channels=12, out_channels=num_classes, crf=model_params['crf'], use_dist = model_params['dist_loss']).to(device) \n",
    "    elif model_name.startswith('UNet-'):\n",
    "        model = unets.UNet(in_channels=12, out_channels=num_classes, crf=model_params['crf']).to(device) \n",
    "    elif model_name.startswith('UNetResNet34-'):\n",
    "        model = unets.UNetResNet34(in_channels=12, out_channels=num_classes, crf=model_params['crf']).to(device) \n",
    "    elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "        model = unets.UNetEfficientNetB0(in_channels=12, out_channels=num_classes, crf=model_params['crf']).to(device) \n",
    "    elif model_name.startswith('UNetConvNext-'):\n",
    "        model = unets.UNetConvNext (in_channels=12, out_channels=num_classes, crf=model_params['crf']).to(device) \n",
    "    elif model_name.startswith('HRNetW18'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w18_small\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW32'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "    elif model_name.startswith('HRNetW48'):\n",
    "        model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "    else:\n",
    "        print(f'Modelo {model_name} não está no param grid. Pulando...')\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(model_params['loss'])\n",
    "    train.train_model(model, \n",
    "                        train_loader, \n",
    "                        val_loader, \n",
    "                        epochs=model_params['epochs'], \n",
    "                        loss_mode = model_params['loss'],\n",
    "                        device = device,\n",
    "                        num_classes = num_classes, \n",
    "                        simulated_batch_size = training_batch_size, #model_params['batch_size'] ,\n",
    "                        patience = model_params['patience'],\n",
    "                        weights = class_weights,\n",
    "                        show_batches = 1, \n",
    "                        save_to = model_name+'.pth')\n",
    "    try:\n",
    "        train.test_model(model, \n",
    "                     checkpoint_path=model_name+'.pth',\n",
    "                     dataloader = test_loader, \n",
    "                     device = device, \n",
    "                     num_classes = num_classes\n",
    "                     ) \n",
    "                     #loss_mode = model_params['loss'], \n",
    "                     #simulated_batch_size = model_params['batch_size'] ,\n",
    "                     #show_batches = 3,\n",
    "                     #yield_predictions = True)\n",
    "    except:\n",
    "        print('ERROR IN TESTING')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
