{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"{prefix} Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"{prefix} Memory Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import src.data.preprocess_data as data\n",
    "import src.training.train_model as train\n",
    "import src.data.view as view\n",
    "import src.models.unets as unets\n",
    "import src.models.hrnets as hrnets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath('..')\n",
    "models_dir = os.path.join(working_dir, 'models')\n",
    "models_names = os.listdir(models_dir)\n",
    "\n",
    "load_model = True\n",
    "infos = {}\n",
    "for model_name in models_names:\n",
    "    print(model_name)\n",
    "    try:\n",
    "        patch_size = int(model_name.split('-')[1])\n",
    "        weighted = 'W.pth' in model_name\n",
    "        if model_name.split('-')[2]=='type':\n",
    "            num_classes = 5\n",
    "        if model_name.split('-')[2]=='binary':\n",
    "            num_classes = 2\n",
    "        crf = False\n",
    "        dist = False\n",
    "        if len(model_name.split('-'))>=4 and model_name.split('-')[3]=='crf':\n",
    "            crf = True\n",
    "        if len(model_name.split('-'))>=4 and model_name.split('-')[3]=='dist':\n",
    "            dist = True\n",
    "            \n",
    "        if load_model:\n",
    "            if model_name.startswith('UNetSmall-'):\n",
    "                model = unets.UNetSmall(in_channels=12, out_channels=num_classes, crf=crf, use_dist=dist).to(device) \n",
    "            if model_name.startswith('UNet-'):\n",
    "                model = unets.UNet(in_channels=12, out_channels=num_classes).to(device) \n",
    "            elif model_name.startswith('UNetResNet34-'):\n",
    "                model = unets.UNetResNet34(in_channels=12, out_channels=num_classes).to(device) \n",
    "            elif model_name.startswith('UNetEfficientNetB0-'):\n",
    "                model = unets.UNetEfficientNetB0(in_channels=12, out_channels=num_classes).to(device) \n",
    "            elif model_name.startswith('UNetConvNext-'):\n",
    "                model = unets.UNetConvNext (in_channels=12, out_channels=num_classes).to(device) \n",
    "            elif model_name.startswith('HRNetW18'):\n",
    "                model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w18\", pretrained=True,).to(device)\n",
    "            elif model_name.startswith('HRNetW32'):\n",
    "                model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w32\", pretrained=True,).to(device)\n",
    "            elif model_name.startswith('HRNetW48'):\n",
    "                model = hrnets.HRNetSegmentation(in_channels= 12, num_classes=num_classes, backbone=\"hrnet_w48\", pretrained=True,).to(device)\n",
    "        ckp_file = os.path.join(models_dir, model_name)\n",
    "        checkpoint = torch.load(ckp_file, weights_only=False)\n",
    "        info = checkpoint['best_epoch_info']\n",
    "        infos[model_name] = info\n",
    "        print(info)\n",
    "    except Exception as e: \n",
    "        print('error loading.')\n",
    "        print(e)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(infos).T\n",
    "df = df[['epoch', 'lr', 'train_acc', 'val_acc', 'train_micro', 'val_micro', 'train_macro', 'val_macro', 'train_time', 'val_time', 'train_memory', 'val_memory']]\n",
    "df = df.applymap(lambda x: x.item() if isinstance(x, torch.Tensor) else x)\n",
    "dft = df[['train_time', 'val_time']].applymap(lambda x: f'{x:.2f}' if isinstance(x, (float, int)) else x)\n",
    "df[['train_time', 'val_time']] = dft\n",
    "df_ = df[['train_acc', 'val_acc', 'train_micro', 'val_micro', 'train_macro', 'val_macro',]].applymap(lambda x: f'{100*x:.2f}' if isinstance(x, (float, int)) else x)\n",
    "df[['train_acc', 'val_acc', 'train_micro', 'val_micro', 'train_macro', 'val_macro',]] = df_\n",
    "df[['lr']]= df[['lr']].applymap(lambda x: f'{x:.8f}' if isinstance(x, (float, int)) else x)\n",
    "col_names = ['Melhor época', 'LR final', 'Acurácia de treino', 'Acurácia de validação', 'Micro F1-Score de treino','Micro F1-Score de validação', \n",
    "             'Macro F1-Score de treino','Macro F1-Score de validação', 'Tempo de treino (época)','Tempo de validação (época)', 'Memória de treino', 'Memória de validação']\n",
    "map = {c1:c2 for c1, c2 in zip(df.columns,col_names)}\n",
    "df = df.rename(columns=map)\n",
    "\n",
    "\n",
    "df = df.sort_index().astype(str)\n",
    "\n",
    "print(df.T.to_latex())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = ['epoch', 'lr', 'train_acc', 'val_acc', 'train_micro', 'val_micro', 'train_time', 'val_time', 'train_memory', 'val_memory']\n",
    "col_names = ['Melhor época', 'LR final', 'Acurácia de treino', 'Acurácia de validação', 'Micro F1-Score de treino','Micro F1-Score de validação', \n",
    "             'Macro F1-Score de treino','Macro F1-Score de validação', 'Tempo de treino (época)','Tempo de validação (época)', 'Memória de treino', 'Memória de validação']\n",
    "for model,v in infos.items():\n",
    "    \n",
    "    print(model)\n",
    "    print([k for k in key])\n",
    "    print([v[k] for k in key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classification_report_from_cm(cm, class_names=None):\n",
    "    n_classes = cm.shape[0]\n",
    "    report = {}\n",
    "    total_samples = np.sum(cm)\n",
    "    support = np.sum(cm, axis=1)  # true samples per class\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        TP = cm[i, i]\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        # TN is not used for per-class precision/recall/F1\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "        if class_names:\n",
    "            label = class_names[i]\n",
    "        else:\n",
    "            label = f\"Class {i}\"\n",
    "            \n",
    "        report[label] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1,\n",
    "            \"support\": support[i]\n",
    "        }\n",
    "    \n",
    "    # Optionally, compute weighted averages:\n",
    "    weighted_precision = np.average(precisions, weights=support)\n",
    "    weighted_recall = np.average(recalls, weights=support)\n",
    "    weighted_f1 = np.average(f1s, weights=support)\n",
    "    \n",
    "    report[\"weighted avg\"] = {\n",
    "        \"precision\": weighted_precision,\n",
    "        \"recall\": weighted_recall,\n",
    "        \"f1-score\": weighted_f1,\n",
    "        \"support\": total_samples\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Example confusion matrix for 3 classes\n",
    "cm = np.array([[50,  2,  3],\n",
    "               [ 4, 45,  1],\n",
    "               [ 5,  2, 40]])\n",
    "\n",
    "report = classification_report_from_cm(cm, class_names=[\"A\", \"B\", \"C\"])\n",
    "for label, metrics in report.items():\n",
    "    print(label, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "infos = dict(sorted(infos.items()))\n",
    "keys = [\"train_f1_C0\", \"train_f1_C1\", \"train_f1_C2\", \"train_f1_C3\", \"train_f1_C4\"]\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "values = [[infos[name][k] for k in keys] for name in names]  # Data for each group\n",
    "save_to = os.path.join(working_dir,\"figs\",\"train_f1.png\")\n",
    "view.plot_horizontal(values, names, \"F1\", \"Treino\", save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = [\"val_f1_C0\", \"val_f1_C1\", \"val_f1_C2\", \"val_f1_C3\", \"val_f1_C4\"]\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "values = [[infos[name][k] for k in keys] for name in names]  # Data for each group\n",
    "save_to = os.path.join(working_dir,\"figs\",\"val_f1.png\")\n",
    "view.plot_horizontal(values, names, \"F1\", \"Validação\", save_to=save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "infos = dict(sorted(infos.items()))\n",
    "\n",
    "\n",
    "\n",
    "keys = [\"val_f1_C0\", \"val_f1_C1\", \"val_f1_C2\", \"val_f1_C3\", \"val_f1_C4\"]\n",
    "keys = [\"train_acc\", \"val_acc\", \"train_micro\", \"val_micro\", \"train_macro\", \"val_macro\"]\n",
    "\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "train_acc = [infos[name]['train_acc'] for name in names]\n",
    "val_acc = [infos[name]['val_acc'] for name in names]\n",
    "train_micro = [infos[name]['train_micro'] for name in names]\n",
    "val_micro = [infos[name]['val_micro'] for name in names]\n",
    "train_macro = [infos[name]['train_macro'] for name in names]\n",
    "val_macro = [infos[name]['val_macro'] for name in names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8)) \n",
    "\n",
    "ax.bar(names, val_acc)\n",
    "ax.bar(names, train_acc)\n",
    "ax.bar(names, train_micro)\n",
    "ax.bar(names, val_micro)\n",
    "ax.bar(names, train_macro)\n",
    "ax.bar(names, val_macro)\n",
    "ax.set_xticklabels(names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = [\"train_acc\", \"val_acc\", \"train_micro\", \"val_micro\", \"train_macro\", \"val_macro\"]\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "save_to = os.path.join(working_dir,\"figs\",\"train_val_metrics.png\")\n",
    "metric_names = [\"Acurácia de treino\", \"Acurácia de validação\", \"F1-score (micro) de treino\", \"F1-score (micro) de validação\", \"F1-score (macro) de treino\", \"F1-score (macro) de validação\"]\n",
    "#view.plot_horizontal_metric_combination(infos, names, keys, metric_names=metric_names, save_to=save_to)\n",
    "#view.plot_vertical_metric_combination(infos, names, keys, metric_names=metric_names, save_to=save_to)\n",
    "suffixes = ['-type-CE.pth', '-type-CEW.pth', '-type-DS-CE.pth', '-type-DS-CEW.pth']\n",
    "view.plot_metric_combination(infos, names, keys, suffixes, metric_names=metric_names, save_to=save_to)\n",
    "print(infos, names, keys, metric_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"train_acc\", \"train_micro\", \"train_macro\", ]\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "save_to = os.path.join(working_dir,\"figs\",\"train_metrics.png\")\n",
    "metric_names = [\"Acurácia de treino\", \"F1-score (micro) de treino\", \"F1-score (macro) de treino\",]\n",
    "view.plot_horizontal_metric_combination(infos, names, keys, metric_names=metric_names, save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"val_acc\", \"val_micro\", \"val_macro\"]\n",
    "names = list(infos.keys())  # X-axis labels\n",
    "save_to = os.path.join(working_dir,\"figs\",\"val_metrics.png\")\n",
    "metric_names = [\"Acurácia de validação\", \"F1-score (micro) de validação\", \"F1-score (macro) de validação\"]\n",
    "view.plot_horizontal_metric_combination(infos, names, keys, metric_names=metric_names, save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info['val_report']['precision']\n",
    "#info['val_report']['recall']\n",
    "precisions = []\n",
    "recalls = []\n",
    "precisions_train = []\n",
    "recalls_train = []\n",
    "\n",
    "#values = [[infos[name][k] for k in keys] for name in names]  # Data for each group\n",
    "\n",
    "for k,v in infos.items():\n",
    "    print(k)\n",
    "    prec=[]\n",
    "    rec = []\n",
    "    prec_t=[]\n",
    "    rec_t = []\n",
    "    for i in range(5):\n",
    "        prec.append(v['val_report']['precision'][f'Class {i}'])\n",
    "        rec.append(v['val_report']['recall'][f'Class {i}'])\n",
    "        prec_t.append(v['train_report']['precision'][f'Class {i}'])\n",
    "        rec_t.append(v['train_report']['recall'][f'Class {i}'])\n",
    "        #print(v['val_report']['precision'][f'Class {i}'])\n",
    "        #print(v['val_report']['recall'][f'Class {i}'])\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    precisions_train.append(prec_t)\n",
    "    recalls_train.append(rec_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = os.path.join(working_dir,\"figs\",\"train_precision.png\")\n",
    "view.plot_horizontal(precisions_train, names, \"Precisão\", \"Treino\", save_to=save_to)\n",
    "view.plot_vertical(precisions_train, names, \"Precisão\", \"Treino\", save_to=save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = os.path.join(working_dir,\"figs\",\"val_precision.png\")\n",
    "view.plot_horizontal(precisions, names, \"Precisão\", \"Validação\", save_to=save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = os.path.join(working_dir,\"figs\",\"train_recall.png\")\n",
    "view.plot_horizontal(recalls_train, names, \"Recall\", \"Treino\", save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = os.path.join(working_dir,\"figs\",\"val_recall.png\")\n",
    "view.plot_horizontal(recalls, names, \"Recall\", \"Validação\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['-type-CE.pth', '-type-CEW.pth', '-type-DS-CE.pth', '-type-DS-CEW.pth']\n",
    "save_to = os.path.join(working_dir,\"figs\",\"train_recall.png\")\n",
    "view.plot_metric(recalls_train, names, suffixes, metric_name=\"Recall\", save_to=save_to)\n",
    "save_to = os.path.join(working_dir,\"figs\",\"val_recall.png\")\n",
    "view.plot_metric(recalls, names, suffixes, metric_name=\"Recall\", save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes:\n",
    "0 -> fundo\n",
    "1 -> loteamento vazio\n",
    "2 -> outros equipamentos\n",
    "3 -> vazio intraurbano\n",
    "4 -> area urbanizada\n",
    "\n",
    "LR dos HRNet muito baixo? mais epocas/paciencia?\n",
    "\n",
    "weighted: ajudou na classe 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
